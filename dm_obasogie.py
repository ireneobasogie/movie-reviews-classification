# -*- coding: utf-8 -*-
"""dm_ompasogkie.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_sLJvoKmHB679Yon1WD3A6zXOfsVlM3U

### 1. Un projet complet

L'archive [`imdb_smol.tar.gz`](data/imdb_smol.tar.gz) (aussi disponible [dans le
dépôt](https://github.com/LoicGrobol/apprentissage-artificiel/blob/main/slides/06-scikit-learn/data/imdb_smol.tar.gz))
contient 602 critiques de films sous formes de fichiers textes, réparties en deux classes :
positives et négatives (matérialisées par des sous-dossiers). Votre mission est de réaliser un
script qui :

- Charge et vectorise ces données
- Entraîne et compare des classifieurs sur ce jeu de données

L'objectif est de déterminer quel type de vectorisation et de modèle semble le plus adapté et quels
hyperparamètres choisir. Vous pouvez par exemple tester des SVM comme ci-dessus, [un modèle de
régression
logistique](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html),
[un arbre de
décision](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html),
[un modèle bayésien
naïf](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html) ou
[une forêt d'arbres de
décision](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html).
"""

# Load libraries
import os
import nltk
import string
import re
import pandas as pd
import numpy as np
from sklearn import metrics
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from collections import defaultdict
from sklearn.metrics import accuracy_score
from sklearn.svm import SVC 
from sklearn.tree import DecisionTreeClassifier 
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression 
from sklearn.preprocessing import OneHotEncoder
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from tqdm.notebook import tqdm
tqdm.pandas()

# Set seed
np.random.seed(500)

# Package specific installs
nltk.download('stopwords')
nltk.download('punkt')

# CONSTANTS
LANG = 'english' # change for model to work on other languages (may still need to update nltk and spacy_sentence_bert)

#load drive path 
from google.colab import drive
drive.mount('/content/drive')
data = defaultdict(list)
path = ['/content/drive/MyDrive/Projet ML/DM perso/neg', '/content/drive/MyDrive/Projet ML/DM perso/pos']

#loading the data in a dataframe and labeling
for i in path:
    for filename in os.listdir(i):
        with open(os.path.join(i, filename), 'r') as f:
            data['Text'].append(f.read())
            if i == path[0]:
                data['Polarity'].append('Negative')
            else:
                data['Polarity'].append('Positive')
df = pd.DataFrame(data)
print(df)

#cleaning the data

def clean_text(text):
    # Remove punctuation
    text = text.translate(str.maketrans("", "", string.punctuation))

    # Remove numbers
    text = re.sub(r"\d+", "", text)

    # Convert to lowercase
    text = text.lower()

    # Remove stop words
    stop_words = set(stopwords.words("english"))
    text = " ".join([word for word in text.split() if word not in stop_words])

    return text

df["Text"] = df["Text"].apply(clean_text)

print(df)

#dataframe shape : rows and columns
print(df.shape)

#assigning the heads
X = df['Text']
y = df['Polarity']

# using the train test split function to split the data
X_train, X_test, y_train, y_test = train_test_split(
  X, y , random_state=104,test_size=0.25, shuffle=True)

# visualising the split of the data
print('X_train : ')
print(X_train.head())
print(X_train.shape)

print('')
print('X_test : ')
print(X_test.head())
print(X_test.shape)
 
print('')
print('y_train : ')
print(y_train.head())
print(y_train.shape)
 
print('')
print('y_test : ')
print(y_test.head())
print(y_test.shape)

# the dataframe cleaned
train_df = pd.DataFrame(data={"x_train":X_train, "y_train":y_train})
test_df = pd.DataFrame(data={"x_test":X_test, "y_test":y_test})
print(train_df.head())
print(test_df.head())

#pos and negs at the y_train data
train_df['y_train'].value_counts().plot.bar()

#pos and negs at the y_test data
test_df['y_test'].value_counts().plot.bar()

# shuffle the DataFrame rows
train_df = train_df.sample(frac = 1)
test_df = test_df.sample(frac = 1)
# Drop Na
train_df = train_df.dropna()
test_df = test_df.dropna()

# Tokenize via vectorization
train_df["x_train_tokens"] = train_df["x_train"].apply(word_tokenize)
test_df["x_test_tokens"] =   test_df["x_test"].apply(word_tokenize)

# Remove stop words
def remove_stop_words(tokens):
  for token in tokens:
      if token in stopwords.words(LANG):
          tokens.remove(token)
  return tokens
train_df["x_train_cleaned"] = train_df["x_train_tokens"].progress_apply(remove_stop_words)
test_df["x_test_cleaned"] = test_df["x_test_tokens"].progress_apply(remove_stop_words)

#visualising the train dataframe
train_df.head()

#visualising the test dataframe
test_df.head()

#installing bert
!pip install spacy_sentence_bert

# loading one of the models listed at https://github.com/MartinoMensio/spacy-sentence-bert/
import spacy_sentence_bert
nlp = spacy_sentence_bert.load_model('en_stsb_distilbert_base')

# converting list of strings to strings, to allow for vectorization
def list_to_string(s):
    # initialize an empty string
    str1 = " "
    # return string 
    return (str1.join(s))
train_df["x_train_final"] = train_df["x_train_cleaned"].apply(list_to_string)
test_df["x_test_final"] = test_df["x_test_cleaned"].apply(list_to_string)

#vectorization
def vectorize(string):
  return nlp(string).vector
train_df["x_train_vector"] = train_df["x_train_final"].progress_apply(vectorize)
test_df["x_test_vector"] = test_df["x_test_final"].progress_apply(vectorize)

# dataframe cleaned + the vectorized column 
print(type(train_df["x_train_final"][0]))
print(train_df["x_train_final"][0])
train_df.head()

# Support Vector Machine (SVM) Classifer
svm = SVC(gamma='auto', verbose=True) 
svm.fit(train_df["x_train_vector"].to_list(),train_df["y_train"].to_list())
y_pred = svm.predict(test_df["x_test_vector"].to_list())
print(f'Our accuracy is: {np.round(accuracy_score(test_df["y_test"].to_list(), y_pred)*100, decimals=4)}%')

# Random Forest Classifer
rf = RandomForestClassifier(max_depth=None, random_state=0)
rf.fit(train_df["x_train_vector"].to_list(),train_df["y_train"].to_list())
y_pred = rf.predict(test_df["x_test_vector"].to_list())
print(f'Our accuracy is: {np.round(accuracy_score(test_df["y_test"].to_list(), y_pred)*100, decimals=4)}%')

# Logistic Regression Classifer
lr = LogisticRegression(max_iter=5000)
lr.fit(train_df["x_train_vector"].to_list(),train_df["y_train"].to_list())
y_pred = lr.predict(test_df["x_test_vector"].to_list())
print(f'Our accuracy is: {np.round(accuracy_score(test_df["y_test"].to_list(), y_pred)*100, decimals=4)}%')

# Naive Bayes Classifer
nb = GaussianNB()
nb.fit(train_df["x_train_vector"].to_list(),train_df["y_train"].to_list())
y_pred = nb.predict(test_df["x_test_vector"].to_list())
print(f'Our accuracy is: {np.round(accuracy_score(test_df["y_test"].to_list(), y_pred)*100, decimals=4)}%')

# Decision Tree Classifer 
dtc = DecisionTreeClassifier()
dtc.fit(train_df["x_train_vector"].to_list(),train_df["y_train"].to_list())
y_pred = dtc.predict(test_df["x_test_vector"].to_list())
print(f'Our accuracy is: {np.round(accuracy_score(test_df["y_test"].to_list(), y_pred)*100, decimals=4)}%')

"""Les meuilleurs classifieurs sont : le SVM Classifier et le Naive Bayes Classifer avec 77.4834% de précision. """